# -*- coding: utf-8 -*-
"""model_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C_UoITnblYN-AOt2Flwb76_Ns7i_FkxJ

# Connect to google drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Get necessaries library"""

!pip install contractions
!pip install wordcloud
import contractions
import pickle as pk
import string
import pandas as pd
import re
from bs4 import BeautifulSoup
import spacy
import unicodedata
nlp = spacy.load('en_core_web_sm')
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pickle as pk

"""# download second movie dataset"""

df = pd.read_csv('/content/drive/MyDrive/nlp/Test/yelp_labelled_v1.csv')

df.info()

"""# remove null columns"""

df = df.drop('Unnamed: 2', axis=1)
df = df.drop('Unnamed: 3', axis=1)
df = df.drop('Unnamed: 4', axis=1)
df = df.drop('Unnamed: 5', axis=1)

df.shape

df.info()

print(df)

"""# Delete rows in sentiment columns with string values and reset indexes"""

df_back = df.copy()
df["sentimen_edit"] = pd.to_numeric(df["sentiment"], errors="coerce")
df.dropna(subset=["sentimen_edit"], inplace=True)
df.reset_index(drop=True, inplace=True)

print(df)

"""# remove duplicates"""

# Check if there are aduplicated values in the data & drop it if found :
duplicated_features=df.duplicated().sum()
print("Number of duplicates ----->>> ",duplicated_features)
df = df.drop_duplicates()
duplicated_features=df.duplicated().sum()
print("Number of duplicates of cleaning it ----->>> ",duplicated_features)

df.reset_index(drop=True, inplace=True)

df_backup = df.copy()
df = df.drop('sentiment', axis=1)



"""# pre-processing only on reviews column"""

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)
df.Review = df['Review'].apply(remove_emoji)

def remove_html_tags(text):
    return BeautifulSoup(text, 'html.parser').get_text()
df.Review = df['Review'].apply(remove_html_tags)


def to_lowercase(text):
    return text.lower()
df.Review = df['Review'].apply(to_lowercase)


def standardize_accented_chars(text):
 return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
df.Review = df['Review'].apply(standardize_accented_chars)


def expand_contractions(text):
    expanded_words = []
    for word in text.split():
       expanded_words.append(contractions.fix(word))
    return ' '.join(expanded_words)
df.Review = df['Review'].apply(expand_contractions)


def remove_url(text):
 return re.sub(r'https?:\S*', '', text)
df.Review = df['Review'].apply(remove_url)



def remove_mentions_and_tags(text):
    text = re.sub(r'@\S*', '', text)
    return re.sub(r'#\S*', '', text)

df.Review = df['Review'].apply(remove_mentions_and_tags)



def remove_special_characters(text):
    # define the pattern to keep
    pat = r'[^a-zA-z0-9.,!?/:;\"\'\s]'
    return re.sub(pat, '', text)
df.Review = df['Review'].apply(remove_special_characters)


def remove_numbers(text):
    pattern = r'[^a-zA-z.,!?/:;\"\'\s]'
    return re.sub(pattern, '', text)
df.Review = df['Review'].apply(remove_numbers)


def remove_punctuation(text):
    return ''.join([c for c in text if c not in string.punctuation])
df.Review = df['Review'].apply(remove_punctuation)


def data_processing(text):
  return re.sub('', '', text)
df.Review = df['Review'].apply(data_processing)

df.Review = df.Review.replace(r'\s+', ' ', regex=True)

print(df)



"""# removing stopwords"""

def remove_stopwords(text):
    filtered_sentence =[]
    doc=nlp(text)
    for token in doc:
        if token.is_stop == False:
          filtered_sentence.append(token.text)
    return ' ' .join(filtered_sentence)
df.Review = df['Review'].apply(remove_stopwords)



"""
# lemmtization"""

def lemmatize(text):
   doc = nlp(text)
   lemmatized_text = []
   for token in doc:
     lemmatized_text.append(token.lemma_)
   return ' '.join(lemmatized_text)
df['Review'] = df.Review.apply(lemmatize)

print(df.info())

df['Review'].astype(str)

plt.figure(figsize = (10,10))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.sentimen_edit == 1.0].Review))
plt.title("Positive Words")
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (10,10))
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.sentimen_edit == 0.0].Review))
plt.title("Negative Words")
plt.imshow(wc , interpolation = 'bilinear')

"""# Download previous train model

"""

model = pk.load(open('/content/drive/MyDrive/nlp/Test_v1/model.pkl','rb'))
scaler = pk.load(open('/content/drive/MyDrive/nlp/Test_v1/scaler.pkl','rb'))

print(df)

number_column = df.shape[1]

col_one_list = df['Review'].tolist()

list_score = []

for i in col_one_list:
  review = i
  review_scale = scaler.transform([review]).toarray()
  result = model.predict(review_scale)
  if result[0] == 0:
    list_score.append (0)
  else:
    list_score.append(1)

series_score = pd.Series( list_score)

print(series_score)

df['score_validate'] = series_score

df.sentimen_edit = df.sentimen_edit.astype(float)
df.score_validate = df.score_validate.astype(float)

df['sentimen_edit'].equals(df['score_validate'])

number_rows = df.shape[0]
k = 0
m= 0
for i in range(number_rows):
  if df.sentimen_edit[i] == df.score_validate[i]:
    k = k + 1
  else:
    m = m + 1

"""# m is incorrect prediction by model

# k is correct prediction by model
"""

final_answer = (k)/(k+m)

print("correct guess = ",k)
print("total review = ",k+m)

"""# calculate percent of corrected guess"""

print("percent correct guess = " ,final_answer)